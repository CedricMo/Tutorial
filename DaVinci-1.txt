LHCb Data Flow
==============

10e11 protons / bunch  -> 10-100 kB /bunch crossing
100 times more D than B mesons
L0: muon and HCAL
We can inject Ne He gas inside the VELO
9:39 : use of FULLSTREAM
Storage : Bookkeeping

Data Format:
before trigger :RAW format
between trigger and Full stream: MDF
after brunel: DST
In Bookekkeeping: DST, mDST

9:45 : difference between DST and mDST
DST: contails all info, for important analysis like BS -> mu mu, data are saved in DST : 150 kB
mDST: 10-100 kB


HLT1 -> Disk: use to calibrate sub detectors

Run2: TURBO: mDST (Tesla appli to convert from MDF to mDST) 
All stripping has to move in the trigger

10:05 : Gaudi

10:11 : MC
We let PYTHIA running till it generates e Bs for ex
Decfiles
Resonances : 
	intermediate step which lead to the same final state ex : K*0

Geant4: simul la propagation dans le detecteur

BOOLE: take true tracks and convert it into detector response
(output in DST or DIGI)
LOAPP : simulate the L0


My storing places:
==================


On LXPLUS
---------
user: up to 10 GB
work: /afs/cern.ch/work/c/cmeaux (/work instead /user) up to 100 GB
EOS: 
2TB of space on a set of hard drives called the EOS service 
+ 2TB on grid storage

On MARLHCB
----------



The LHCb Software
=================

lb-run : launch the LHCb software (based on Gaudi framework)

lb-run Project/Version Application(of the project) option.py

eg:
lb-run DaVinci/v41r2
lb-run LHCbDirac/latest
lb-run Bender

Gaudi
-----

Usually, you will work with one of the LHCb software projects that are based on Gaudi. One of the most important ones is DaVinci, which provides lots of Algorithms and Tools for physics analysis.

lb-run DaVinci/v41r2 gaudirun.py
lb-run DaVinci/latest gaudirun.py

This will run the gaudirun.py command using version v41r2 of DaVinci. gaudirun.py is a script that sets up the EventLoop. 

During this run, DaVinci didn't do anything: We didn't specify any algorithms to run or any data to run over. Usually, you will write an option file (e.g. options.py) and specify it as an argument to gaudirun.py:

lb-run DaVinci/v41r2 gaudirun.py options.py
An option.py is just a regular Python script that specifies how to set things up in the software. Many of the following lessons will teach you how to do something with DaVinci by showing you how to write or extend an options.py.

Do you want to start a shell that already contains the LHCb environment, so you don't have to use lb-run at each time? 
lb-run DaVinci/v41r2 $SHELL


Bookkeeping
-----------

Simulation Condition, open it and change it to Event type. (famous 8 digit number)

FOR MC:
Sim08e : lastest version of simulation, usually the better
We also have to choose the version of the digitisation and what configuration of the trigger and reconstruction we want to have in the simulated sample. Usually there is only one choice for these, which makes choosing easier.

We also have to select a version of the stripping. Choose any as long as it contains the word Flagged.

Flagged and filtered samples:
In the usual data-taking flow, the trigger and stripping are run in filtering mode, whereby events that don't pass any trigger line or any stripping line are thrown away. In the simulation, it's often useful to keep such events so that the properties of the rejected events can be studied. The trigger and stripping are then run in flagging mode, such that the decisions are only recorded for later inspection. Filtered Monte Carlo can be produced for analyses that need lots of events.


At the bottom of your browser window there is a text field next to a green "plus" symbol. You can directly enter a path here to navigate there directly. For example you could go straight to: evt+std://MC/2012/27163003/Beam4000GeV-2012-MagDown-Nu2.5-Pythia8/ by typing this path and pressing the Go button.

Once you see the list of DST File, click on SAVE and use python format

Download a file from the grid
----------------------------

Once we have downloaded the python file (called data.py in this note) from the grid, which is just a collection of Logical File Name (LFN), go to your lxplus $HOME, and run:

lb-run LHCbDIRAC lhcb-proxy-init (load Dirac Software and initiate the proxy)

lb-run dirac-dms-get-file data.py

.. note:: dirac-dms-get-file (and the other dirac-dms-* scripts) is actually able to extract the LFNs from any file and download them for you (in the directory where you have launched the command. 

You can also download the DST file individually:
lb-run LHCbDIRAC dirac-dms-get-file LFN:/lhcb/MC/2012/ALLSTREAMS.DST/00035742/0000/00035742_00000002_1.allstreams.dst
  
NB: You can also read files remotely instead of downloading them, see:
https://lhcb.github.io/first-analysis-steps/files-from-grid.html



Explore DST file in an interactive python session
-------------------------------------------------

Data are stored in files called DSTs, which are processed by DaVinci to make nTuples. However you can also explore them interactively from a python session.

To explore the DST we could have simply done:

lb-run Bender/latest bender 00035742_00000002_1.allstreams.dst

Bender also provides a useful command dst-dump, which is a quick way of figuring out what objects are present on a DST

it put you in ipython:  
  ls(): 
	get('Rec/Header')
  list and get the TES locations

pRec p stands for pack

ls('/Event/AllStreams/Phys/D2hhCompleteEventPromptDst2D2RSLine/Particles')

seekForData('/Event/AllStreams/Phys/D2hhCompleteEventPromptDst2D2RSLine/Particles')

is equivalent to:
while True:
	run(1)
	particles = get('seekForData('/AllStreams/Phys/D2hhCompleteEventPromptDst2D2RSLine/Particles')
	if particles: break


In [3]: get('/Event/AllStreams/Phys/D2hhCompleteEventPromptDst2D2RSLine/Particles').size()
Out[3]: 1L

In [4]: dst = get('/Event/AllStreams/Phys/D2hhCompleteEventPromptDst2D2RSLine/Particles')[0] -> 1st particle of the list

dst is a particle now!

Loki Functors
-------------

LoKi: Loops and Kinematics
	collections of functors: some kinf of objects that can be used as a function


PT is a functor
PT(dst)

In [11]: MeV
Out[11]: 1.0
to check unit
In [13]: PT(dst)/GeV
Out[13]: 6.821827474548151

VCHI2

In [14]: VFASPF
Out[14]: LoKiPhys.functions.LoKi::Particles::VFunAsPFun

In [17]: VFASPF(VCHI2)(dst)
Out[17]: 2.6047301292419434

In [18]: VCHI2(dst.endVertex())
Out[18]: 2.6047301292419434

In [19]: dst.endVertex()
Out[19]:
{ position  : (2.0205,1.2204,-30.8167)
covMatrix :
[      1.96308     1.64515    -18.6218
       1.64515     1.37898    -15.6075
      -18.6218    -15.6075     176.659 ]
chi2      : 2.60473
nDoF      : 1
extraInfo : [ ] }{ Technique         :	Unknown
OutgoingParticles : [0x448a9a20, 0x448ac768]
TES=/Event/AllStreams/Phys/D2hhCompleteEventPromptDst2D2RSLine/decayVertices }

In [17]: VFASPF(VCHI2)(dst)
In [18]: VCHI2(dst.endVertex())
Out[17]: 2.6047301292419434
SAME OUTPUT!

Create new functors:
In [21]: (PX + PY)
Out[21]:  (PX+PY)

In [22]: sumpxpy = (PX + PY)
In [23]: sumpxpy(dst)
Out[23]: -9610.71

In [24]: CHILD
Out[24]: LoKiPhys.functions.LoKi::Particles::ChildFunction

In [25]: M12
Out[25]: MASS(1,2)

In [25]: M12
Out[25]: MASS(1,2)

In [26]: M12(dst)
Out[26]: 2007.5664579600018
In [28]: M(dst)
Out[28]: 2007.2584537126636

In [29]: MM(dst)
Out[29]: 2007.258
MM:Magic Masse


In [31]: CHILD(M,1)(dst)
Out[31]: 1862.412615399713

In [33]: CHILD(M, '[D*(2010)+ -> (D0 -> ^K- pi+) pi+]CC')(dst)
Out[33]: 493.6770040977003
The ^ stipulates which particle interest us


CC: Charge Conjugate

 0 |->D*(2010)+                    M/PT/E/PX/PY/PZ: 2.0073/ 6.8218/ 50.11/-5.226/-4.384/ 49.61 [GeV]  #  0
                                       EndVertex  X/Y/Z: 2.021/ 1.22 /-30.82 [mm]  Chi2/nDoF  2.605/1 #  0
 1    |->D0                        M/PT/E/PX/PY/PZ: 1.8624/ 6.4521/ 47.44/-4.939/-4.152/ 46.96 [GeV]  #  0
                                       EndVertex  X/Y/Z:0.2911/-0.2378/-14.38 [mm]  Chi2/nDoF 0.4039/1 #  0
 2       |->K-                     M/PT/E/PX/PY/PZ: 0.4937/ 2.8013/ 25.45/-1.799/-2.147/ 25.29 [GeV]  # 19
 2       |->pi+                    M/PT/E/PX/PY/PZ: 0.1396/ 3.7258/ 21.99/-3.141/-2.004/ 21.67 [GeV]  # 22
 1    |->pi+                       M/PT/E/PX/PY/PZ: 0.1396/ 0.3701/ 2.678/-0.2873/-0.2333/ 2.649 [GeV]  # 10

DecFile (read by EvtGen)
PHSP: PhaseSpace
VSS: Vector Scalar 

From DST to Tuples (DaVinci) 
-----------------------------

Looping event-by-event over a file and inspecting interesting quantities with LoKi functors is great for exploration: to checking that the file contains the candidates you need, that the topology makes sense, and so on. It's impractical for most cases, though, where you want all the candidates your trigger/stripping line produced, which could be tens of millions of decays. In these cases we use DaVinci, the application for analysing high-level information such as tracks and vertices, which we'll look at in this lesson to produce a ROOT ntuple.



TimingAuditor.T...   INFO EVENT LOOP                                        |    24.719 |    54.643 |    0.354   11930.4   689.61 |     300 |    16.393 |
TimingAuditor.T...   INFO  DaVinciEventSeq                                  |    24.006 |    49.022 |    0.342   11930.3   688.74 |     300 |    14.707 |
TimingAuditor.T...   INFO   DaVinciInitAlg                                  |     0.069 |     0.072 |    0.034       1.9     0.12 |     300 |     0.022 |
TimingAuditor.T...   INFO   FilteredEventSeq                                |    23.929 |    48.938 |    0.299   11930.3   688.74 |     300 |    14.682 |
TimingAuditor.T...   INFO    DaVinciEventInitSeq                            |     0.013 |     0.008 |    0.003       0.6     0.03 |     300 |     0.003 |
TimingAuditor.T...   INFO     PhysInitSeq                                   |     0.000 |     0.001 |    0.000       0.0     0.00 |     300 |     0.000 |
TimingAuditor.T...   INFO     AnalysisInitSeq                               |     0.003 |     0.001 |    0.000       0.0     0.00 |     300 |     0.000 |
TimingAuditor.T...   INFO    DaVinciAnalysisSeq                             |    23.913 |    48.926 |    0.293   11930.2   688.74 |     300 |    14.678 |
TimingAuditor.T...   INFO     DaVinciUserSequence                           |    23.899 |    48.908 |    0.289   11930.2   688.74 |     300 |    14.673 |
TimingAuditor.T...   INFO      TupleDstToD0Pi_D0ToKpi                       |    18.270 |    39.282 |    0.002   11782.0   680.23 |     300 |    11.785 |
TimingAuditor.T...   INFO     MonitoringSequence                            |     0.006 |     0.013 |    0.000       3.6     0.21 |     300 |     0.004 |
TimingAuditor.T...   INFO   LumiSeq                                         |     0.003 |     0.004 |    0.002       0.0     0.00 |     300 |     0.001 |
TimingAuditor.T...   INFO    EventAccount                                   |     0.000 |     0.001 |    0.001       0.0     0.00 |     300 |     0.000 |
TimingAuditor.T...   INFO *     AllStreams_PsAndVsUnpack                    |     2.655 |     4.986 |    0.045     221.1    20.58 |     971 |     4.842 |
TimingAuditor.T...   INFO *       AllStreams_pRec_Vertex_Primary_Converter  |    12.342 |    23.586 |    0.175     220.7    40.69 |      93 |     2.194 |
TimingAuditor.T...   INFO *        UnpackTrack                              |    13.008 |    23.213 |    0.193     219.4    38.58 |      93 |     2.159 |
TimingAuditor.T...   INFO *      createODIN                                 |     0.836 |     4.628 |    0.047     142.5    16.80 |     184 |     0.852 |
TimingAuditor.T...   INFO *      L0DUFromRaw                                |     6.999 |    10.862 |    1.014      20.7    13.93 |       2 |     0.022 |
TimingAuditor.T...   INFO *      Hlt1DecReportsDecoder                      |   297.955 |   830.917 |    0.059    1661.8  1175.01 |       2 |     1.662 |
TimingAuditor.T...   INFO *      Hlt2DecReportsDecoder                      |     0.999 |     0.800 |    0.217       1.4     0.83 |       2 |     0.002 |
TimingAuditor.T...   INFO *     UnpackRecVertex                             |     1.000 |     1.247 |    0.797       1.7     0.64 |       2 |     0.002 |
TimingAuditor.T...   INFO ----------------------------------------------------------------------------------------------------------------------
NTupleSvc            INFO NTuples saved successfully
ApplicationMgr       INFO Application Manager Finalized successfully
ApplicationMgr       INFO Application Manager Terminated successfully

Only 2 evts passed the stripping selection we provided

TBrowser doesn't workon lxplus !!! But work on marlhcb !! WHY ??


GANGA: 
-------

take our minimal DaVinci job and run it on the grid.
ganga is a program which you can use to interact with your grid jobs.


SetupGanga
ganga
j = Job()

# Define applications
j.application = DaVinci(version='v41r2')

j.name = "the_name"

j.comment = "what the job do"


k.jobs(883).copy()

# Show the output of the jobs
j.peek() list of foiles in the output directory
j.peek("stdout")  standard output
j.peek("rootfile.root, "root.exe") open the root file

gangadir/....../numberjob/output

jobs(number of the job).remove()

si bcp de job

jobs in ganga to get the list of jobs
of go on the DIRAC webpage, and go on Job Monitor



Ganga In [16]: for i in range(0,6):
    if jobs(i).status == "new":
        jobs(i).remove()
    if jobs(i).status == "failed":
        jobs(i).resubmit()
    # option to resubmit automatically jobs :

exit from ganga Ctrl+D


*****
Jeudi
*****

Branch, Tree
============

kinematic fit: decay tree fitter 9:59

Interactive Root Commands
-------------------------

_file0->ls()
_file0->cd("TupleDstToD0pi_D0ToKpi")
TFile *_file0 = TFile::Open("Bd_Dst-3pipi0,munu_Trig_CBS.root")
TTree *T = DecayTree


2nd part MORE GANGA: Storing large files on EOS
-------------------


 [15] â†’ ganga

*** Welcome to Ganga ***
Version: 6.2.3
Documentation and support: http://cern.ch/ganga
Type help() or help('index') for online help.

This is free software (GPL), and you are welcome to redistribute it
under certain conditions; type license() for details.


INFO     reading config file /afs/cern.ch/lhcb/software/releases/GANGA/GANGA_v602r3/install/ganga/python/GangaLHCb/LHCb.ini
INFO     reading config file /afs/cern.ch/sw/ganga/install/config/LHCb/6-2-0/CERN.ini
INFO     reading config file /afs/cern.ch/user/c/cmeaux/.gangarc
INFO     Using LHCbDirac version v8r3p10

[10:58:06]
Ganga In [1]: J = job()
ERROR    Error: name 'job' is not defined

[10:58:14]
Ganga In [2]: J = Job()

[10:58:17]
Ganga In [3]: myApp = GaudiExec()

myApp = prepareGaudiExec('DaVinci', 'v41r2', myPath='.')

J.application = myApp

J.application.options = ['opts.py']
Configure the job
J.backend = Dirac()
 
J.application.readInputData('data.py')


Create a job
USe prepareGaudiExec() method with DaVinci v41r2
Set options file
Set input data using the readInputData
Set output file with LocalFile()
Set backend to Dirac()
Submit Job

Once the job is completed:
J.outputfiles[0]
or jobs(6).outputfiles[0]

In ganga: to run our script
%ganga ganga_job.py

if this ERROR MESSAGE:


ERROR    Error in uploading file jobScripts-c4f4c897-3f40-467b-bf18-c86a6f3b3747.tar.gz : {'SARA-USER': {'Errno': 0, 'Message': 'Failed to put file to Storage Element. Permission denied ( 13 : SE.isValid: Write access not currently permitted.)', 'OK': False, 'CallStack': ['  File "<stdin>", line 471, in <module>\n', '  File "<string>", line 457, in <module>\n', '  File "<string>", line 116, in uploadFile\n', '  File "/afs/cern.ch/lhcb/software/releases/LHCBDIRAC/LHCBDIRAC_v8r3p10/LHCbDIRAC/Interfaces/API/DiracLHCb.py", line 97, in addFile\n    printOutput = printOutput )\n', '  File "/afs/cern.ch/lhcb/software/releases/DIRAC/DIRAC_v6r15p19/DIRAC/Interfaces/API/Dirac.py", line 1252, in addFile\n    return self._errorReport( \'Problem during putAndRegister call\', result[\'Message\'] )\n', '  File "/afs/cern.ch/lhcb/software/releases/DIRAC/DIRAC_v6r15p19/DIRAC/Core/Base/API.py", line 74, in _errorReport\n    return S_ERROR( message )\n']}}

do:
j.backend.settings['BannedSites'] = ["LCG.SARA.NL"]


j.outputfiles[0].accessURL()

URL: ROOT:....
Can use the URL in ROOT scrit !! ;D

LCG: LHC Computation Grid
DVntuples: DaVinci ntuples/tuples

jobs(45).peek(): list path of the output of the job
j.id
j.status
j.subjobs


root -l URL


Run Job in parallel: Splitter
we can close the connection, if the output are directed to the grid (DiracFile), not it the output need to talk with ganga

queues.add(j.submit) without bracket (bracket call it)
to put in 